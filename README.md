# OpenSign: Real-time Sign Language Recognition

<여기에 대충 로고 사인 넣자 없으면 그냥 비우기>

---

## :star: 프로젝트 소개

**OpenSign** 프로젝트는 수어를 배우길 원하는 모든 사람들이 쉽게 사용할 수 있는 모션인식 기반 수어 교육 프로그램 입니다.
구글에서 제공하는 모션 인식 오픈소스 라이브러리인 "MediaPipe"와 RNN 계열의 가벼운 신경망 모델인 "GRU", 학습에 효과적인 가중치 분배를 위해 "Attention" 함수를 활용했습니다.
사용자에게서 웹캠으로 영상데이터(mp4 형식)를 입력 받으면 이를 좌표 데이터(npy 형식)로 전환하고, 이를 학습된 모델에 거쳐 정확도를 제공합니다. 

**OpenSign** 프로젝트는 사용자가 반복적인 수어 동작을 함으로써 특정 단어에 대한 사용자의 수어 기억력 향상이라는 기대효과, 
웹으로 제공되어 누구든지 쉽게 접근 가능하다는 기대효과가 있습니다. 또한, 사용자들의 좌표데이터를 모아 한국수어(KSL)의 데이터 증가에 기여할 수 있습니다. 
마지막으로, 수어 모델 학습 파이프라인을 사용하여 수어뿐만 아니라 다양한 동작을 프레임별 npy 배열로 저장해 다른 분야에 확장하여 적용할 수 있는 효과도 있습니다.

---

## :star: 프로젝트 구성



### [1]-1 지화 인식 시스템 아키텍처
1. **데이터 수집**: 자체 촬영 사진 활용 ➡️ 자음: 14개(쌍자음 제외) & 모음: 21개
2. **전처리 & Feature 추출**: 
3. **모델 학습**: 
4. **실시간 예측**: 
5. **GUI 출력**: 

### [1]-2 수화 인식 시스템 아키텍처
1. **데이터 수집**: AIhub에서 특정 카테고리에 맞는 단어(총 30단어) 영상데이터 수집


     (링크: https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=264)


2. **전처리 & Feature 추출: `(frame, dimension)` 특징 벡터 추출**


   2-1. 손가락 KeyPoints


   2-2. 손가락 사이 관절 각도 차원


   2-3. 얼굴 대비 손 위치



5. **모델 학습**: GRU + Attention 학습 구조로 모델 학습
6. **실시간 예측**: 90프레임을 사용자 웹캠에서 수집 후 해당하는 단어 예측
7. **GUI 출력**: 결과를 시각적으로 활용하여 수어 학습에 활용

---

### [2] 데이터셋 설명

- **영상 데이터**: KETI 한국 수어 데이터셋(수화에서 활용) + 자체 촬영 사진(지화에서 활용)
- **Feature 구성 (총 3개) - `총 152차원`**

   1. 손가락 KeyPoints (왼손 21관절 * 3차원 + 오른손 21관절 * 3차원) 총 `126차원`: Mediapipe에서 기본값으로 인식되는 양손 Keypoints를 3차원 구조로 확장했습니다.
   2. 손가락 사이 관절 각도 차원 (왼손 10차원 + 오른손 10차원) 총 `20차원`: 손가락의 모양을 더 세부적으로 학습하기 위해서 추가했습니다.
   3. 얼굴 대비 손 위치 (코 <-> 왼쪽 손목 3차원 + 코 <-> 오른쪽 손목 3차원) 총 `6차원`: 다양하게 수어를 구분하기 위해 얼굴과 손 사이 위치관계를 정의합니다.
---

### [3]-1 지화 인식 모델 설명


### [3]-2 수화 인식 모델 설명
- **GRU (Gated Recurrent Unit)**  
  - 프레임 순서 별 좌표데이터를 차례대로 학습하기 위해 가벼운 시계열 신경망 구조인 **GRU** 모델을 선택했습니다.
  - LSTM 대비 파라미터가 적어 빠른 실시간 추론에 적합합니다.
- **Attention Mechanism**  
  - 각 프레임에서 **손 특징에 가중치**를 부여합니다.
  - 수어 동작이 다양해서 다양한 손 모양에 사람이 직접 수동으로 가중치를 주기 어려워, 자동으로 가중치를 부여하도록 **Attention Mechanism**을 선택했습니다.

---

### [4] 모델 학습 & 실시간 예측

1. 학습 데이터: 영상 데이터에서 추출한 데이터셋 `(90 frame, 152 dimension)` feature 로 벡터화
2. GRU + Attention 모델 학습  
3. 실시간 추론 과정
   - `s` 키 입력 → 90프레임 수집  
   - Mediapipe로 keypoint 추출 & feature 변환  
   - 모델 예측 → 단어 확률 출력  
   - GUI에 결과 표시, 손 검출 실패 시 `"검출된 손 없음"` 메시지

---

### [5] GUI 기반 수어 교육 프로그램

- 예측 결과를 **시각화된 카드**로 출력  
- 단어별 **하이라이트 영상**과 함께 학습 가능  
- 사용자 친화적 **NanumGothic 폰트 & 컬러 UI** 적용

---

## :star: 시연 영상

![Demo](docs/demo.gif)

> 실제 카메라 기반 실시간 수어 인식 시연 장면

---

## :star: OpenSign 사용 방법

여기에는 순서대로 실시간만 넣자
---

## :star: 모델 학습 코드

여기에는 모델 학습 코드랑 방법 순서로 넣자
---

## :star: 사용한 오픈소스

- [Mediapipe](https://github.com/google/mediapipe) – 손·얼굴 keypoint 추출
- [PyTorch](https://pytorch.org/) – GRU + Attention 모델 학습
- [OpenCV](https://opencv.org/) – 영상 처리 및 GUI 출력

---

## :star: 사용한 LLM

- **ChatGPT** – 코드 설계, 파이프라인 개선, README 문서화
- **LLM 활용 예시**  
  - 전처리 & 학습 파이프라인 설계 지원  
  - Attention 적용 및 실시간 추론 개선

---

## :star: 팀원 소개

| 이름       | 역할                  | GitHub/블로그 |
|-----------|---------------------|----------------|
| 홍길동     | 프로젝트 기획 / GUI 설계 | [GitHub](https://github.com/username) |
| 김수어     | 모델 학습 / 전처리       | [Blog](https://blog.example.com) |
| 박인식     | 실시간 추론 / 데이터셋 구축 | [GitHub](https://github.com/username) |

---

## :star: 라이선스

MIT License
