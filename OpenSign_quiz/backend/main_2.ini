import os
import io
import base64
import pickle
import torch
import numpy as np
from PIL import Image
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
import mediapipe as mp
from model_v2 import KeypointGRUModelV2

# ───── 기본 설정 ─────
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_DIR = os.path.join(BASE_DIR, "models")
FRAME_TARGET = 90

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ───── 모델/라벨맵 캐시 ─────
model_cache = {}
label_map_cache = {}

# ───── Mediapipe 설정 ─────
mp_holistic = mp.solutions.holistic
holistic = mp_holistic.Holistic(static_image_mode=True, model_complexity=1, min_detection_confidence=0.5)

# ───── 서버 시작 시 모델 로드 ─────
@app.on_event("startup")
def load_all_models():
    for category in ["가족", "응급상황", "시간"]:
        model_path = os.path.join(MODEL_DIR, f"{category}_model.pth")
        label_path = os.path.join(MODEL_DIR, f"{category}_label_map.pkl")

        with open(label_path, "rb") as f:
            label_map = pickle.load(f)
            label_map_cache[category] = label_map

        model = KeypointGRUModelV2(input_dim=152, attn_dim=146, num_classes=len(label_map)).to(DEVICE)
        model.load_state_dict(torch.load(model_path, map_location=DEVICE))
        model.eval()
        model_cache[category] = model

# ───── 입력 데이터 모델 ─────
class PredictRequest(BaseModel):
    category: str
    frames: List[str]  # base64 PNG 90장

# ───── /labels API ─────
@app.get("/labels")
def get_labels(category: str = Query(...)):
    label_map = label_map_cache.get(category)
    if not label_map:
        return {"error": f"라벨맵 없음: {category}"}

    first_key = list(label_map.keys())[0]
    first_val = list(label_map.values())[0]

    if isinstance(first_key, str):
        labels = [k for k, _ in sorted(label_map.items(), key=lambda x: x[1])]
    else:
        labels = [v for k, v in sorted(label_map.items(), key=lambda x: x[0])]

    return labels

# ───── 특징 추출 함수 ─────
def calculate_relative_hand_coords(hand_kpts):
    if np.all(hand_kpts == 0): return hand_kpts
    return hand_kpts - hand_kpts[0]

def calculate_finger_angles(hand_kpts):
    if np.all(hand_kpts == 0): return np.zeros(10)
    angles = []
    fingers = {'thumb':[1,2,3,4], 'index':[5,6,7,8], 'middle':[9,10,11,12], 'ring':[13,14,15,16], 'pinky':[17,18,19,20]}
    for joints in fingers.values():
        for i in range(len(joints)-2):
            a, b, c = hand_kpts[joints[i]], hand_kpts[joints[i+1]], hand_kpts[joints[i+2]]
            v1, v2 = a - b, c - b
            cos = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2)+1e-6)
            angles.append(np.arccos(np.clip(cos, -1.0, 1.0)))
    return np.array(angles)

def calculate_hand_face_relation(lh, rh, face):
    nose = face[1] if np.any(face) else np.zeros(3)
    lw = lh[0] if np.any(lh) else np.zeros(3)
    rw = rh[0] if np.any(rh) else np.zeros(3)
    return np.concatenate([lw - nose, rw - nose])

def extract_feature(image_np):
    results = holistic.process(image_np)
    face = np.array([[l.x,l.y,l.z] for l in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros((468,3))
    lh = np.array([[l.x,l.y,l.z] for l in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros((21,3))
    rh = np.array([[l.x,l.y,l.z] for l in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros((21,3))

    rel_lh = calculate_relative_hand_coords(lh).flatten()
    rel_rh = calculate_relative_hand_coords(rh).flatten()
    angles_lh = calculate_finger_angles(lh)
    angles_rh = calculate_finger_angles(rh)
    rel_feat = calculate_hand_face_relation(lh, rh, face)

    return np.concatenate([rel_lh, rel_rh, angles_lh, angles_rh, rel_feat])

# ───── /predict API ─────
@app.post("/predict")
def predict(req: PredictRequest):
    category = req.category
    frames = req.frames

    if category not in model_cache:
        return {"error": "지원하지 않는 카테고리입니다."}
    if len(frames) != FRAME_TARGET:
        return {"error": "프레임 수가 90이 아닙니다."}

    sequence = []
    for frame_b64 in frames:
        image = Image.open(io.BytesIO(base64.b64decode(frame_b64.split(",")[1]))).convert("RGB")
        image_np = np.array(image)
        feature = extract_feature(image_np)
        sequence.append(feature)

    x = torch.tensor(np.array(sequence), dtype=torch.float32).unsqueeze(0).to(DEVICE)
    model = model_cache[category]
    label_map = label_map_cache[category]
    idx_to_label = {v: k for k, v in label_map.items()} if isinstance(list(label_map.keys())[0], str) else label_map

    with torch.no_grad():
        pred = torch.softmax(model(x), dim=-1)
        top = torch.argmax(pred, dim=-1).item()
        prob = pred[0, top].item()

    return {"label": idx_to_label[top], "prob": round(prob, 4)}
